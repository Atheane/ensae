{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2A.ml - Texte et machine learning\n",
    "\n",
    "Revue de méthodes de [word embedding](https://en.wikipedia.org/wiki/Word_embedding) statistiques (~ [NLP](https://en.wikipedia.org/wiki/Natural_language_processing)) ou comment transformer une information textuelle en vecteurs dans un espace vectoriel (*features*) ? Deux exercices sont ajoutés à la fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"my_id_menu_nb\">run previous cell, wait for 2 seconds</div>\n",
       "<script>\n",
       "function repeat_indent_string(n){\n",
       "    var a = \"\" ;\n",
       "    for ( ; n > 0 ; --n) {\n",
       "        a += \"    \";\n",
       "    }\n",
       "    return a;\n",
       "}\n",
       "var update_menu_string = function(begin, lfirst, llast, sformat, send, keep_item) {\n",
       "    var anchors = document.getElementsByClassName(\"section\");\n",
       "    if (anchors.length == 0) {\n",
       "        anchors = document.getElementsByClassName(\"text_cell_render rendered_html\");\n",
       "    }\n",
       "    var i,t;\n",
       "    var text_menu = begin;\n",
       "    var text_memo = \"<pre>\\nlength:\" + anchors.length + \"\\n\";\n",
       "    var ind = \"\";\n",
       "    var memo_level = 1;\n",
       "    var href;\n",
       "    var tags = [];\n",
       "    var main_item = 0;\n",
       "    for (i = 0; i <= llast; i++) {\n",
       "        tags.push(\"h\" + i);\n",
       "    }\n",
       "\n",
       "    for (i = 0; i < anchors.length; i++) {\n",
       "        text_memo += \"**\" + anchors[i].id + \"--\\n\";\n",
       "\n",
       "        var child = null;\n",
       "        for(t = 0; t < tags.length; t++) {\n",
       "            var r = anchors[i].getElementsByTagName(tags[t]);\n",
       "            if (r.length > 0) {\n",
       "child = r[0];\n",
       "break;\n",
       "            }\n",
       "        }\n",
       "        if (child == null){\n",
       "            text_memo += \"null\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        if (anchors[i].hasAttribute(\"id\")) {\n",
       "            // when converted in RST\n",
       "            href = anchors[i].id;\n",
       "            text_memo += \"#1-\" + href;\n",
       "            // passer à child suivant (le chercher)\n",
       "        }\n",
       "        else if (child.hasAttribute(\"id\")) {\n",
       "            // in a notebook\n",
       "            href = child.id;\n",
       "            text_memo += \"#2-\" + href;\n",
       "        }\n",
       "        else {\n",
       "            text_memo += \"#3-\" + \"*\" + \"\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        var title = child.textContent;\n",
       "        var level = parseInt(child.tagName.substring(1,2));\n",
       "\n",
       "        text_memo += \"--\" + level + \"?\" + lfirst + \"--\" + title + \"\\n\";\n",
       "\n",
       "        if ((level < lfirst) || (level > llast)) {\n",
       "            continue ;\n",
       "        }\n",
       "        if (title.endsWith('¶')) {\n",
       "            title = title.substring(0,title.length-1).replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").replace(\"&\", \"&amp;\")\n",
       "        }\n",
       "\n",
       "        if (title.length == 0) {\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        while (level < memo_level) {\n",
       "            text_menu += \"</ul>\\n\";\n",
       "            memo_level -= 1;\n",
       "        }\n",
       "        if (level == lfirst) {\n",
       "            main_item += 1;\n",
       "        }\n",
       "        if (keep_item != -1 && main_item != keep_item + 1) {\n",
       "            // alert(main_item + \" - \" + level + \" - \" + keep_item);\n",
       "            continue;\n",
       "        }\n",
       "        while (level > memo_level) {\n",
       "            text_menu += \"<ul>\\n\";\n",
       "            memo_level += 1;\n",
       "        }\n",
       "        text_menu += repeat_indent_string(level-2) + sformat.replace(\"__HREF__\", href).replace(\"__TITLE__\", title);\n",
       "    }\n",
       "    while (1 < memo_level) {\n",
       "        text_menu += \"</ul>\\n\";\n",
       "        memo_level -= 1;\n",
       "    }\n",
       "    text_menu += send;\n",
       "    //text_menu += \"\\n\" + text_memo;\n",
       "    return text_menu;\n",
       "};\n",
       "var update_menu = function() {\n",
       "    var sbegin = \"\";\n",
       "    var sformat = '<li><a href=\"#__HREF__\">__TITLE__</a></li>';\n",
       "    var send = \"\";\n",
       "    var keep_item = -1;\n",
       "    var text_menu = update_menu_string(sbegin, 2, 4, sformat, send, keep_item);\n",
       "    var menu = document.getElementById(\"my_id_menu_nb\");\n",
       "    menu.innerHTML=text_menu;\n",
       "};\n",
       "window.setTimeout(update_menu,2000);\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jyquickhelper import add_notebook_menu\n",
    "add_notebook_menu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données\n",
    "\n",
    "Nous allons travailler sur des données twitter collectées avec le mot-clé macron : [tweets_macron_sijetaispresident_201609.zip](https://github.com/sdpython/ensae_teaching_cs/raw/master/src/ensae_teaching_cs/data/data_web/tweets_macron_sijetaispresident_201609.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>776066992054861825</td>\n",
       "      <td>776067660979245056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb_user_mentions</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb_extended_entities</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb_hashtags</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_hashtags</th>\n",
       "      <td>, SiJétaisPrésident</td>\n",
       "      <td>, SiJétaisPrésident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annee</th>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delimit_mention</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_str</th>\n",
       "      <td>7.76067e+17</td>\n",
       "      <td>7.76068e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_mention</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retweet_count</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>favorite_count</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_extended_entities</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>#SiJétaisPrésident se serait la fin du monde.....</td>\n",
       "      <td>#SiJétaisPrésident je donnerai plus de vacance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb_user_photos</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb_urls</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb_symbols</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <td>Wed Sep 14 14:36:04 +0000 2016</td>\n",
       "      <td>Wed Sep 14 14:38:43 +0000 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delimit_hash</th>\n",
       "      <td>, 0, 18</td>\n",
       "      <td>, 0, 18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        0  \\\n",
       "index                                                  776066992054861825   \n",
       "nb_user_mentions                                                        0   \n",
       "nb_extended_entities                                                    0   \n",
       "nb_hashtags                                                             1   \n",
       "geo                                                                   NaN   \n",
       "text_hashtags                                         , SiJétaisPrésident   \n",
       "annee                                                                2016   \n",
       "delimit_mention                                                       NaN   \n",
       "lang                                                                   fr   \n",
       "id_str                                                        7.76067e+17   \n",
       "text_mention                                                          NaN   \n",
       "retweet_count                                                           4   \n",
       "favorite_count                                                          3   \n",
       "type_extended_entities                                                 []   \n",
       "text                    #SiJétaisPrésident se serait la fin du monde.....   \n",
       "nb_user_photos                                                          0   \n",
       "nb_urls                                                                 0   \n",
       "nb_symbols                                                              0   \n",
       "created_at                                 Wed Sep 14 14:36:04 +0000 2016   \n",
       "delimit_hash                                                      , 0, 18   \n",
       "\n",
       "                                                                        1  \n",
       "index                                                  776067660979245056  \n",
       "nb_user_mentions                                                        0  \n",
       "nb_extended_entities                                                    0  \n",
       "nb_hashtags                                                             1  \n",
       "geo                                                                   NaN  \n",
       "text_hashtags                                         , SiJétaisPrésident  \n",
       "annee                                                                2016  \n",
       "delimit_mention                                                       NaN  \n",
       "lang                                                                   fr  \n",
       "id_str                                                        7.76068e+17  \n",
       "text_mention                                                          NaN  \n",
       "retweet_count                                                           5  \n",
       "favorite_count                                                          8  \n",
       "type_extended_entities                                                 []  \n",
       "text                    #SiJétaisPrésident je donnerai plus de vacance...  \n",
       "nb_user_photos                                                          0  \n",
       "nb_urls                                                                 0  \n",
       "nb_symbols                                                              0  \n",
       "created_at                                 Wed Sep 14 14:38:43 +0000 2016  \n",
       "delimit_hash                                                      , 0, 18  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ensae_teaching_cs.data import twitter_zip\n",
    "df = twitter_zip(as_df=True)\n",
    "df.head(n=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5088, 20)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5000 tweets n'est pas assez pour tirer des conclusions mais cela donne une idée. On supprime les valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5087, 2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df[[\"retweet_count\", \"text\"]].dropna()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construire une pondération\n",
    "\n",
    "Le texte est toujours délicat à traiter. Il n'est pas toujours évident de sortir d'une information binaire : un mot est-il présent ou pas. Les mots n'ont aucun sens numérique. Une liste de tweets n'a pas beaucoup de sens à part les trier par une autre colonne : les retweet par exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>842.0</td>\n",
       "      <td>#SiJetaisPresident travailler moins pour gagne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>816.0</td>\n",
       "      <td>#SiJetaisPresident je ferais revenir l'été ave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2627</th>\n",
       "      <td>529.0</td>\n",
       "      <td>#SiJetaisPresident le mcdo livrerai à domicile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>289.0</td>\n",
       "      <td>#SiJetaisPresident les devoirs ça serait de re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198</th>\n",
       "      <td>276.0</td>\n",
       "      <td>#SiJetaisPresident ? Président c'est pour les...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      retweet_count                                               text\n",
       "2038          842.0  #SiJetaisPresident travailler moins pour gagne...\n",
       "2453          816.0  #SiJetaisPresident je ferais revenir l'été ave...\n",
       "2627          529.0     #SiJetaisPresident le mcdo livrerai à domicile\n",
       "1402          289.0  #SiJetaisPresident les devoirs ça serait de re...\n",
       "2198          276.0   #SiJetaisPresident ? Président c'est pour les..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values(\"retweet_count\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sans cette colonne qui mesure la popularité, il faut trouver un moyen d'extraire de l'information. On découpe alors en mots et on constuire un modèle de langage : les [n-grammes](https://fr.wikipedia.org/wiki/N-gramme). Si un tweet est constitué de la séquence de mots $(m_1, m_2, ..., m_k)$. On définit sa probabilité comme :\n",
    "\n",
    "$$P(tweet) = P(w_1, w_2) P(w_3 | w_2, w_1) P(w_4 | w_3, w_2) ... P(w_k | w_{k-1}, w_{k-2})$$\n",
    "\n",
    "Dans ce cas, $n=3$ car on suppose que la probabilité d'apparition d'un mot ne dépend que des deux précédents. On estime chaque n-grammes comme suit :\n",
    "\n",
    "$$P(c | a, b) = \\frac{ \\# (a, b, c)}{ \\# (a, b)}$$\n",
    "\n",
    "C'est le nombre de fois où on observe la séquence $(a,b,c)$ divisé par le nombre de fois où on observe la séquence $(a,b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation\n",
    "\n",
    "Découper en mots paraît simple ``tweet.split()`` et puis il y a toujours des surprises avec le texte, la prise en compte des tirets, les majuscules, les espaces en trop. On utilse un *tokenizer* dédié : [TweetTokenizer](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.casual.TweetTokenizer) ou un tokenizer qui prend en compte le langage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#sijétaisprésident',\n",
       " 'se',\n",
       " 'serait',\n",
       " 'la',\n",
       " 'fin',\n",
       " 'du',\n",
       " 'monde',\n",
       " '...',\n",
       " 'mdr',\n",
       " '😂']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(preserve_case=False)\n",
    "tokens = tknzr.tokenize(data.loc[0, \"text\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grammes\n",
    "\n",
    "* [N-Gram-Based Text Categorization: Categorizing Text With Python](http://blog.alejandronolla.com/2013/05/20/n-gram-based-text-categorization-categorizing-text-with-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None, None, None, '#sijétaisprésident'),\n",
       " (None, None, '#sijétaisprésident', 'se'),\n",
       " (None, '#sijétaisprésident', 'se', 'serait'),\n",
       " ('#sijétaisprésident', 'se', 'serait', 'la'),\n",
       " ('se', 'serait', 'la', 'fin'),\n",
       " ('serait', 'la', 'fin', 'du'),\n",
       " ('la', 'fin', 'du', 'monde'),\n",
       " ('fin', 'du', 'monde', '...'),\n",
       " ('du', 'monde', '...', 'mdr'),\n",
       " ('monde', '...', 'mdr', '😂'),\n",
       " ('...', 'mdr', '😂', None),\n",
       " ('mdr', '😂', None, None),\n",
       " ('😂', None, None, None)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "generated_ngrams = ngrams(tokens, 4, pad_left=True, pad_right=True)\n",
    "list(generated_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1 : calculer des n-grammes sur les tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage\n",
    "\n",
    "Tous les modèles sont plus stables sans les stop-words, c'est-à-dire tous les mots présents dans n'importe quel documents et qui n'apporte pas de sens (à, de, le, la, ...). Souvent, on enlève les accents, la ponctuation... Moins de variabilité signifie des statistiques plus fiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 : nettoyer les tweets\n",
    "\n",
    "Voir [stem](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure de graphe\n",
    "\n",
    "On cherche cette fois-ci à construire des coordonnées pour chaque tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matrice d'adjacence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une option courante est de découper chaque expression en mots puis de créer une matrice *expression x mot* ou chaque case indique la présence d'un mot dans une expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5087, 11924)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "counts = count_vect.fit_transform(data[\"text\"])\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On aboutit à une matrice sparse ou chaque expression est représentée à une vecteur ou chaque 1 représente l'appartenance d'un mot à l'ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[:5,:5].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#SiJétaisPrésident se serait la fin du monde... mdr 😂'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0,\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[0,:].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### td-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce genre de technique produit des matrices de très grande dimension qu'il faut réduire. On peut enlever les mots rares ou les mots très fréquents. [td-idf](https://fr.wikipedia.org/wiki/TF-IDF) est une technique qui vient des moteurs de recherche. Elle construit le même type de matrice (même dimension) mais associe à chaque couple (document - mot) un poids qui dépend de la fréquence d'un mot globalement et du nombre de documents contenant ce mot.\n",
    "\n",
    "$$idf(t) = \\log \\frac{\\# D}{\\#\\{d \\; | \\; t \\in d \\}}$$\n",
    "\n",
    "Où :\n",
    "\n",
    "* $\\#D$ est le nombre de tweets\n",
    "* $\\#\\{d \\; | \\; t \\in d \\}$ est le nombre de tweets contenant le mot $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(t,d)$ est le nombre d'occurences d'un mot $t$ dans un document $d$.\n",
    "\n",
    "$$tf(t,d) = \\frac{1}{2} + \\frac{1}{2} \\frac{f(t,d)}{\\max_{t' \\in d} f(t',d)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On construit le nombre $tfidf(t,f)$\n",
    "\n",
    "$$tdidf(t,d) = tf(t,d) idf(t)$$\n",
    "\n",
    "Le terme $idf(t)$ favorise les mots présent dans peu de documents, le terme $tf(t,f)$ favorise les termes répétés un grand nombre de fois dans le même document. On applique à la matrice précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5087, 11924)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "res = tfidf.fit_transform(counts)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6988143126521051"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0,:].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3 : tf-idf sans mot-clés\n",
    "\n",
    "La matrice ainsi créée est de grande dimension. Il faut trouver un moyen de la réduire avec [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec\n",
    "\n",
    "* [word2vec From theory to practice](http://hen-drik.de/pub/Heuer%20-%20word2vec%20-%20From%20theory%20to%20practice.pdf)\n",
    "* [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\n",
    "* [word2vec](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "\n",
    "Cet algorithme part d'une répresentation des mots sous forme de vecteur en un espace de dimension N = le nombre de mots distinct. Un mot est représenté par $(0,0, ..., 0, 1, 0, ..., 0)$. L'astuce consiste à réduire le nombre de dimensions en compressant avec une ACP, un réseau de neurones non linéaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#sijétaisprésident',\n",
       " 'se',\n",
       " 'serait',\n",
       " 'la',\n",
       " 'fin',\n",
       " 'du',\n",
       " 'monde',\n",
       " '...',\n",
       " 'mdr',\n",
       " '😂']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [tknzr.tokenize(_) for _ in data[\"text\"]]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-10-03 00:42:12,299 : INFO : collecting all words and their counts\n",
      "2016-10-03 00:42:12,299 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2016-10-03 00:42:12,327 : INFO : collected 13263 word types from a corpus of 76467 raw words and 5087 sentences\n",
      "2016-10-03 00:42:12,392 : INFO : min_count=1 retains 13263 unique words (drops 0)\n",
      "2016-10-03 00:42:12,392 : INFO : min_count leaves 76467 word corpus (100% of original 76467)\n",
      "2016-10-03 00:42:12,484 : INFO : deleting the raw counts dictionary of 13263 items\n",
      "2016-10-03 00:42:12,484 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2016-10-03 00:42:12,484 : INFO : downsampling leaves estimated 56079 word corpus (73.3% of prior 76467)\n",
      "2016-10-03 00:42:12,484 : INFO : estimated required memory for 13263 words and 100 dimensions: 17241900 bytes\n",
      "2016-10-03 00:42:12,559 : INFO : resetting layer weights\n",
      "2016-10-03 00:42:12,893 : INFO : training model with 3 workers on 13263 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5\n",
      "2016-10-03 00:42:12,893 : INFO : expecting 5087 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-10-03 00:42:13,376 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-10-03 00:42:13,376 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-10-03 00:42:13,391 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-10-03 00:42:13,391 : INFO : training on 382335 raw words (280377 effective words) took 0.5s, 575841 effective words/s\n"
     ]
    }
   ],
   "source": [
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-10-03 00:42:18,089 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('bien', 0.9993120431900024),\n",
       " ('mon', 0.9993098974227905),\n",
       " ('ministre', 0.9993038177490234),\n",
       " ('3', 0.9993023872375488),\n",
       " ('/', 0.9992935657501221),\n",
       " ('ma', 0.999293327331543),\n",
       " ('dans', 0.9992847442626953),\n",
       " ('merde', 0.9992846250534058),\n",
       " ('€', 0.9992778301239014),\n",
       " ('#bayrou', 0.9992730617523193)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_word(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"fin\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07167088,  0.01037968, -0.0347001 ,  0.13618709, -0.04936545,\n",
       "        0.06919333, -0.07977331, -0.04044997, -0.10819082, -0.12842277,\n",
       "        0.08534056, -0.07823293, -0.09098279,  0.01913173, -0.23570576,\n",
       "        0.03220233,  0.0213078 ,  0.24803311,  0.03564203, -0.03161603,\n",
       "       -0.03495153, -0.07290805,  0.03283146, -0.00746943,  0.07626498,\n",
       "        0.08364875,  0.00777306, -0.07886092,  0.12784876,  0.07973223,\n",
       "        0.06004526,  0.05176223, -0.06843602, -0.1104833 , -0.10198253,\n",
       "        0.0043983 , -0.09931083, -0.05931143, -0.04970795, -0.02507968,\n",
       "       -0.24674726,  0.02844877,  0.23148981, -0.01295278,  0.01698331,\n",
       "       -0.09990757, -0.10728305, -0.09666982,  0.07170945,  0.06428482,\n",
       "       -0.06271251, -0.00600671,  0.20045315,  0.06947709,  0.19523463,\n",
       "        0.05860612, -0.00956623,  0.05660482, -0.04068514, -0.02396445,\n",
       "        0.10675795, -0.08158956, -0.01152411, -0.04703977,  0.09980039,\n",
       "        0.10392339, -0.07677256, -0.00409548,  0.06592534, -0.00480576,\n",
       "        0.00601762, -0.09637805,  0.0136368 , -0.07772852, -0.08013346,\n",
       "       -0.16120504, -0.03272396, -0.02562772,  0.07080878,  0.01316294,\n",
       "       -0.26966235,  0.0906961 , -0.16877754,  0.06177418,  0.09502917,\n",
       "       -0.05076392,  0.2041503 , -0.07089066,  0.09621479,  0.02082463,\n",
       "       -0.06236167,  0.09625127, -0.08881571,  0.19672391,  0.03073362,\n",
       "       -0.07315189,  0.0742472 , -0.03126399,  0.05871766,  0.13382684], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"fin\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging\n",
    "\n",
    "L'objectif est de tagger les mots comme déterminer si un mot est un verbe, un adjectif ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grammar\n",
    "\n",
    "Voir [html.grammar](http://www.nltk.org/api/nltk.html#module-nltk.grammar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRF\n",
    "\n",
    "Voir [CRF](http://www.nltk.org/api/nltk.tag.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMM\n",
    "\n",
    "Voir [HMM](http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.hmm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Une fois qu'on a des coordonnées, on peut faire plein de choses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA\n",
    "\n",
    "* [Latent Dirichlet Application](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)\n",
    "* [LatentDirichletAllocation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=1000)\n",
    "tfidf = tfidf_vectorizer.fit_transform(data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5087, 1000)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_topics=10, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "             n_jobs=1, n_topics=10, perp_tol=0.1, random_state=0,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['avoir', 'bac', 'bah']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tf_feature_names[100:103]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "gratuit mcdo supprimerai école soir kebab macdo kfc domicile cc\n",
      "Topic #1:\n",
      "macron co https de la est le il et hollande\n",
      "Topic #2:\n",
      "sijetaispresident je les de la et le des en pour\n",
      "Topic #3:\n",
      "notaires eu organiserais mets carte nouveaux journées installation cache créer\n",
      "Topic #4:\n",
      "sijetaispresident interdirais les je ballerines la serait serais bah de\n",
      "Topic #5:\n",
      "ministre de sijetaispresident la je premier mort et nommerais président\n",
      "Topic #6:\n",
      "cours le supprimerais jour sijetaispresident lundi samedi semaine je vendredi\n",
      "Topic #7:\n",
      "port interdirait démissionnerais promesses heure rendrai ballerine mes changement christineboutin\n",
      "Topic #8:\n",
      "seraient sijetaispresident gratuits aux les nos putain éducation nationale bonne\n",
      "Topic #9:\n",
      "bordel seront légaliserai putes gratuites pizza mot virerais vitesse dutreil\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda, tf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4 : LDA\n",
    "\n",
    "Recommencer en supprimant les stop-words pour avoir des résultats plus propres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
